### 两阶段提交协议
    分布式事务是指会设计到操作多个数据库的事务，在分布式系统中，各个节点之间
    在物理上相互独立，通过网络进行沟通和协调
    
    XA就是X/Open DTP定义的交易中间件与数据库之间的接口规范(即接口函数)，
    交易中间件用它来通知数据库事务的开始，结束以及提交，回滚等。XA接口函数
    由数据库厂商提供
    
    二阶段提交(Two-phaseCommit)是指，在计算机网络以及数据库领域内，为了使基于分布式系统
    架构下的所有节点在进行事务提交时保持一致性而设计的一种算法(Algorithm)。通常，二阶段提
    交也被称为是一种协议(Protocol))。在分布式系统中，每个节点虽然可以知晓自己的操作时成功
    或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事
    务的 ACID 特性，需要引入一个作为协调者的组件来统一掌控所有节点(称作参与者)的操作结果并
    最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。因此，
    二阶段提交的算法思路可以概括为：
        参与者将操作成败通知给协调者，再由协调者根据所有参与者的反馈情报决定
        各参与者是否要提交操作还是中止操作
        
    
    1. 准备阶段
        事务协调者(事务管理器)给每个参与者(资源管理器)发送 Prepare 消息，每个参与者要么直接返回
        失败(如权限验证失败)，要么在本地执行事务，写本地的 redo 和 undo 日志，但不提交，到达一
        种“万事俱备，只欠东风”的状态。
        
    2. 提交阶段
        如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚(Rollback)消息；否则，
        发送提交(Commit)消息；参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过
        程中使用的锁资源。(注意:必须在最后阶段释放锁资源)
        
    缺点：
        1. 同步阻塞问题：
            执行过程中，所有参与节点都是实物阻塞型的
            
        2. 单点故障
            由于协调者的重要性，一旦协调者发生故障，参与者会一直阻塞下去
            
        3. 数据不一致(脑裂问题)
            在二阶段提交的阶段二中，当协调者向参与者发送 commit 请求之后，发生了局部网络异
            常或者在发送 commit 请求过程中协调者发生了故障，导致只有一部分参与者接受到了
            commit 请求。于是整个分布式系统便出现了数据部一致性的现象(脑裂现象)。
            
        4. 二阶段无法解决的问题(数据状态不确定)
            协调者再发出 commit 消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那
            么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道
            事务是否被已经提交。
            
    

### 三阶段提交协议
    与两阶段提交不同的是
    1. 引入超时机制，同时在协调者和参与者中都引入超时机制
    2. 在第一阶段和第二阶段中插入一个准备阶段，保证了在最后提交阶段之前
    各参与节点的状态是一致的，也就是说，除了引入超时机制之外，3PC把2PC
    的准备阶段再次一分为二，这样三阶段提交就有
    CanCommit，PreCommit，DoCommit三个阶段
    
    1. CanCommit阶段
        协调者向参与者发送commit请求，参与者如果可以提交就返回yes响应，
        否则返回no响应
        
    2. PreCommit阶段
        协调者根据参与者的反应情况来决定是否可以继续进行，有以下两种可能。假如协调者从所有的
        参与者获得的反馈都是 Yes 响应，那么就会执行事务的预执行假如有任何一个参与者向协调者发送
        了 No 响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。 
        
    3. DoCommit阶段
        该阶段进行真正的事务提交，主要包含 1.协调者发送提交请求 2.参与者提交事务 3.参与者响应反
        馈（ 事务提交完之后，向协调者发送 Ack 响应。）4.协调者确定完成事务。
        
### 柔性事务
       CAP
       BASE理论：
        BA：Basically Available
            基本可用
        S：Soft State
            柔性状态(软状态)
        E：Eventual Consistency
            最终一致性
        
        通常所说的柔性事务分为：
            两阶段型，补偿型，异步确定型，最大努力通知型几种
            
        两阶段型：
            分布式两阶段提交
            
        补偿型：
            TCC型事务(Try/Confirm/Cancel)可以归为补偿型
        
            WS-BusinessActivity 提供了一种基于补偿的 long-running 的事务处理模型。服务器 A 发起事务，
            服务器 B 参与事务，服务器 A 的事务如果执行顺利，那么事务 A 就先行提交，如果事务 B 也执行
            顺利，则事务 B 也提交，整个事务就算完成。但是如果事务 B 执行失败，事务 B 本身回滚，这时
            事务 A 已经被提交，所以需要执行一个补偿操作，将已经提交的事务 A 执行的操作作反操作，恢
            复到未执行前事务 A 的状态。这样的 SAGA 事务模型，是牺牲了一定的隔离性和一致性的，但是
            提高了 long-running 事务的可用性
            
        异步确定型：
            通过将一系列同步的事务操作变为基于消息执行的异步操作，避免了分布式
            事务中的同步阻塞操作的影响
            
        最大努力通知型：
            这是分布式事务中要求最低的一种，也可以通过消息中间件实现，与前面异步
            确定型操作不同的一点是，在消息MQ Server投递到消费者之后，允许在达到
            最大重试次数之后正常结束事务
            
   
   
### CAP
    一致性(C)：Consisitency
        在分布式系统中的所有数据备份，在同一时刻是否同样的值。(等同于所有节点访问
        同一份数据副本)
        
    可用性(A)：Availability
        在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。
        (对数据更新具备高可用性)
        
   分区容错性(P)：Partition tolerance
        以实际效果而言，分区相当于对通信的时限要求，系统如果不能在时限内
        达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择
        
        
### 一致性算法
    1. Paxos
        Paxos 算法解决的问题是一个分布式系统如何就某个值（决议）达成一致。一个典型的场景是，
        在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点执行相同的操作序列，那么
        他们最后能得到一个一致的状态。为保证每个节点执行相同的命令序列，需要在每一条指令上执
        行一个“一致性算法”以保证每个节点看到的指令一致。zookeeper 使用的 zab 算法是该算法的
        一个实现。 在 Paxos 算法中，有三种角色：Proposer，Acceptor，Learners
        
        Paxos三种角色：
            Proposer，Acceptor，Learners
            
            Proposer：
                只要Proposer发的提案被半数以上Acceptor接受，Proposer就认为该
                提案里的value被选定了
                
            Acceptor：
                只要Acceptor接受了某个提案，Acceptor就认为该提案里的value被选定了
                
            Learner：
                Acceptor告诉Learner哪个value被选定，Learner就认为那个value被选定
                
                
        Paxos算法分为两个阶段：
            阶段一(准leader确定)
            1. Proposer选择一个提案编号为N，然后向半数以上的Acceptor发送编号为N的
            Prepare请求
            2. 如果一个Acceptor收到一个编号为N的Prepare请求，且N大于该Acceptor已经
            响应过的所有Prepare请求的编号，那么它就会将它已经接受过的编号最大的提案
            (如果有的话)作为响应阶段反馈给Proposer，同时该Acceptor承诺不再接受任何
            编号小于N的提案
            
            阶段二(leader确认)：
            1. 如果Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应
            那么它就会发送一个针对[N,V]提案的Acceptor请求给半数以上的Acceptor，
            注意：V就是收到的响应中编号最大的提案的value，如果响应中不包含任何提案，
            那么V就由Proposer自己决定
            2. 如果Acceptor收到一个针对编号为N的提案的Acceptor请求，只要该Acceptor没有对
            编号大于N的Prepare请求作出过响应，它就接受该提案
            
### ZAB
    ZAB(Zookeeper Atomic Broadcast， Zookeeper 原子消息广播协议)协议包括两种基本的模式：
        崩溃恢复和消息广播
        
    1. 当整个服务框架在启动过程中，或是当Leader服务器出现网络中断崩溃退出与重启
    等异常情况时，ZAB就会进入恢复模式并选举产生新的Leader服务器
    2. 当选举产生了新的Leader服务器，同时集群中已经有过半的机器与该Leader服务器
    完成了状态同步之后，ZAB协议就会退出崩溃恢复模式，进入消息广播模式
    3. 当有新的服务器加入到集群中去，如果此时集群中已经存在一个Leader服务器在负责进行
    消息广播，那么新加入的服务器会自动进入数据恢复模式，找到Leader服务器，并与其
    进行数据同步，然后一起参与到消息广播流程中区
    
    以上其实大致经历了三个步骤：
        1. 崩溃模式：主要就是Leader选举过程
        2. 数据同步：Leader服务器与其他服务器进行数据同步
        3. 消息广播：Leader服务器将数据发送给其他服务器
        
### Raft
    与Paxos不同Raft强调的是易懂，Raft和Paxos一样只要保证n/2 + 1节点正常就能够提供服务；
    raft把算法流程分为三个子问题，选举(Leader election)，日志复制(Log replication)，
    安全性(Safety)三个子问题
    
    角色：
        Raft把集群中的节点分为三种状态：
            Leader，Follower，Candidate
        Raft运行时提供服务的时候只存在与Leader与Follower两种状态
        Leader(领导者-日志管理)：
            负责日志的同步管理，处理来自客户端的请求，与Follower保持着heartBeat的联系
            
        Follower(追随者-日志同步)：
            刚启动时所有节点为Follower状态，响应Leader的日志同步请求，响应Candidate的请求
            把请求到Follower的事务转发给Leader
            
        Candidate(候选者-负责选票)
            负责选举投票，Raft刚启动时由一个节点从Follower转为Candidate发起选举，选举出
            Leader后从Candidate转为Leader状态
            
        Term(任期)
            在Raft中使用了一个可以理解为周期(第几届，任期)的概念，用Term作为一个周期，
            每个Term都是一个连续递增的编号，每一轮选举都是一个Term周期，在一个Term中
            只能产生一个Leader，当某个节点收到的请求中Term比当前Term小时则拒绝该请求
            
        Election(选举)
            选举定时器
                Raft的选举由定时器来触发，每个节点的选举定时器时间都是不一样的，开始时
                状态都为Follower某个节点定时器触发选举后Term递增，状态由Follower转为
                Cadidate，向其它节点发起RequestVote RPC请求，这时候有三种可能的情况发生
                
                    1. RequestVote请求接收到n/2 + 1(过半数)个节点的投票，从Candidate转为Leader
                    向其它节点发送heartBeat以保持Leader的正常运转
                    2. 在此期间如果收到其它节点发送过来的AppendEntries RPC请求，如该节点的Term
                    大则当前节点转为Follower，否则保持Candadate聚聚该请求
                    3. Election timeout发生则Term递增，重新发起选举
                    
                在一个 Term 期间每个节点只能投票一次，所以当有多个 Candidate 存在时就会出现每个
                Candidate 发起的选举都存在接收到的投票数都不过半的问题，这时每个 Candidate 都将 Term
                递增、重启定时器并重新发起选举，由于每个节点中定时器的时间都是随机的，所以就不会多次
                存在有多个 Candidate 同时发起投票的问题。
                 在 Raft 中当接收到客户端的日志（事务请求）后先把该日志追加到本地的 Log 中，然后通过
                heartbeat 把该 Entry 同步给其他 Follower，Follower 接收到日志后记录日志然后向 Leader 发送
                ACK，当 Leader 收到大多数（n/2+1）Follower 的 ACK 信息后将该日志设置为已提交并追加到
                本地磁盘中，通知客户端并在下个 heartbeat 中 Leader 将通知所有的 Follower 将该日志存储在
                自己的本地磁盘中。     
                
### Raft协议和ZAB协议的区别
    相同点：
        1. 采用quorum来确定整个系统的一致性，这个quorum一般实现是集群中半数里上的服务器
        2. zookeeper里还提供了带权重的quorum实现
        3. 都由leader来发起写操作
        4. 都采用心跳检测存活性
        5. leader election都采用先到先得的投票方式
        
    不同点：
        1. zab用的是epoch和count的组合来唯一表示一个值，而raft用的是term和index
        2. zab的follower在投票给一个leader之前必须和leader的日志达成一致，而raft的follower
        则简单地说是谁的term高就投票给谁
        3. raft协议的心跳是从leader到follower，而zab协议则相反
        4. raft协议数据只有单向的从leader到follower(成为leader的条件之一就是拥有最新的log)
        
    而 zab 协议在 discovery 阶段, 一个 prospective leader 需要将自己的 log 更新为 quorum 里面
    最新的 log,然后才好在 synchronization 阶段将 quorum 里的其他机器的 log 都同步到一致     
    
### NWR
    N：在分布式存储系统中，有多少分备份数据
    W：代表依次成功的更新操作要求至少有w分数据写入成功
    R：代表依次成功的读数据操作要求至少有R份数据成功读取
    
    NWR值的不同组合会产生不同的一致性效果，当W+R>N 的时候，整个系统对于客户端来讲能保
    证强一致性。而如果 R+W<=N，则无法保证数据的强一致性。以常见的 N=3、W=2、R=2 为例：
    
    N=3，表示，任何一个对象都必须有三个副本（Replica），W=2 表示，对数据的修改操作
    （Write）只需要在 3 个 Replica 中的 2 个上面完成就返回，R=2 表示，从三个对象中要读取到 2
    个数据对象，才能返回。   
                            
                            
### Gossip
    Gossip 算法又被称为反熵（Anti-Entropy），熵是物理学上的一个概念，代表杂乱无章，而反熵
    就是在杂乱无章中寻求一致，这充分说明了 Gossip 的特点：在一个有界网络中，每个节点都随机
    地与其他节点通信，经过一番杂乱无章的通信，最终所有节点的状态都会达成一致。每个节点可
    能知道所有其他节点，也可能仅知道几个邻居节点，只要这些节可以通过网络连通，最终他们的
    状态都是一致的，当然这也是疫情传播的特点。
    
### 一致性 Hash
    一致性哈希算法(Consistent Hashing Algorithm)是一种分布式算法，常用于负载均衡。
    Memcached client 也选择这种算法，解决将 key-value 均匀分配到众多 Memcached server 上
    的问题。它可以取代传统的取模操作，解决了取模操作无法应对增删 Memcached Server 的问题
    (增删 server 会导致同一个 key,在 get 操作时分配不到数据真正存储的 server，命中率会急剧下
    降)。
    
### 一致性 Hash 特性
    1. 平衡性(Balance)：平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，
    这样可以使得所有的缓冲空间都得到利用
    2.  单调性(MOnotonicity)：单调性是指如果已经有一些内容通过哈希分派到了
    相应的缓冲中，又有新的缓冲加入到系统中，哈希的结果应能够保证原有已分配的
    内容可以被映射到新的缓冲中区，而不会被映射到旧的缓冲集合中的其他缓冲区，
    容易看到，上面的简单求余算法 hash(object) % N难以满足单调性的要求
    平滑性(Smoothness)：平滑性是指缓存服务器的数据平滑改变和缓存对象的平滑
    改变是一致的

### 一致性原理
     1. 构建唤醒hash空间：
        考虑通常的hash算法都是将value映射到一个32位的key值，也即是0~2^32 - 1次方
        的数值空间；
        我们可以将这个空间想象成一个首(0)尾(2^32 - 1)相接的圆环
     
     2. 把需要缓存的内容(对象)映射到hash空间
        接下来考虑4个对象 object1 ~ object4，通过hash函数计算出的hash值key在环上的
        分布
     
     3. 把服务器(节点)映射到hash空间
        Consistent hashing的基本思想就是将对象和cache都映射到同一个hash数值空间中，
        并且使用相同的hash算法，一般的方法可以使用服务器(节点)机器的IP地址或者机器
        名作为hash输入
     
     4. 把对象映射到服务节点
        现在服务节点和对象都已经通过同一个hash算法映射到hash数值空间中了，首先确定
        对象hash值在环上的位置，从此位置沿环顺时针"行走"，第一台遇到的服务器就是
        其应该定位到的服务器     
        
     5. 考察cache的变动
        通过hash然后求余的方法带来的最大问题就在于不能满足单调性，当cache有所变动时，
        cache会失效
     
        1. 移除cache：
            考虑假设cache B挂掉了，根据上面讲到的映射方法，这时受影响的将仅是哪些
            沿cache B逆时针遍历知道下一个cache(cache C)之间的对象
            
        2. 添加cache：
            再考虑添加一台新的cache D的情况，这时受影响的将仅是那些沿cache D逆时针遍历
            知道下一个cache之间的对象，将这些对象重新映射到cache D上即可
            
     6. 虚拟节点
        hash 算法并不是保证绝对的平衡，如果 cache 较少的话，对象并不能被均匀的映射到 cache 上，
        为了解决这种情况， consistent hashing 引入了“虚拟节点”的概念，它可以如下定义：
        
        虚拟节点（ virtual node ）是实际节点在 hash 空间的复制品（ replica ），一实际个节点对应了
        若干个“虚拟节点”，这个对应个数也成为“复制个数”，“虚拟节点”在 hash 空间中以 hash 
        值排列
        
        仍以仅部署 cache A 和 cache C 的情况为例。现在我们引入虚拟节点，并设置“复制个数”为 2 ，
        这就意味着一共会存在 4 个“虚拟节点”， cache A1, cache A2 代表了 cache A； cache C1, 
        cache C2 代表了 cache C 。此时，对象到“虚拟节点”的映射关系为：
        objec1->cache A2 ； objec2->cache A1 ； objec3->cache C1 ； objec4->cache C2 ；
        因此对象 object1 和 object2 都被映射到了 cache A 上，而 object3 和 object4 映射到了 cache 
        C 上；平衡性有了很大提高
                
            
    
        
    